{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import yaml, json, box\n",
    "import util.misc as utils\n",
    "from model import build_model\n",
    "from engine import evaluate, train_one_epoch\n",
    "from dataloader.dataset import CustomDataset\n",
    "\n",
    "def train(args):\n",
    "    device = torch.device(args.scheduler.device)\n",
    "\n",
    "    model, criterion, postprocessors = build_model(args)\n",
    "    model.to(device)\n",
    "    \n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('number of params:', n_parameters)\n",
    "    param_dicts = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": args.scheduler.lr_backbone,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.scheduler.lr,\n",
    "                                  weight_decay=args.scheduler.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.scheduler.lr_drop)\n",
    "    \n",
    "    if args.model.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.model.frozen_weights, map_location='cpu')\n",
    "        model.detr.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "    if args.scheduler.resume:\n",
    "        if args.scheduler.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.scheduler.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.scheduler.resume, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        if 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            args.scheduler.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    ## start ##\n",
    "    dataset = CustomDataset(data_root = DATA_ROOT, csv_file=csv_file, transform= t, infer=False)\n",
    "    data_loader_train = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    print(\"Start training\")\n",
    "    for epoch in range(args.scheduler.start_epoch, args.scheduler.epochs):\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "            args.scheduler.clip_max_norm)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        utils.save_on_master({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'args': args,\n",
    "        }, args.scheduler.save_path + 'checkpoint.pth')\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
