{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM88nFAqS4j7oHGSqJlx/ea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gApzE3QlZjV2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as T\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class Backbone(nn.Module):\n","    def __init__(self):\n","        pass\n","    def foward(self):\n","        pos_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)  # positional embeddings\n","\n","class Transformer(nn.Module):\n","    def __init__(self, num_layers=6, d_model=256, d_ff=2048, dropout=0.1):\n","        super(Transformer, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=d_model, dim_feedforward=d_ff, nhead=1, dropout=dropout),\n","            num_layers=num_layers)\n","        self.decoder = nn.TransformerDecoder(\n","            nn.TransformerEncoderLayer(d_model=d_model, dim_feedforward=d_ff, nhead=2, dropout=dropout),\n","            num_layers=num_layers)\n","\n","    def forward(self, src, query):\n","        key_value = self.encoder(src)\n","        output = self.decoder(query, key_value)\n","        return output\n","\n","class PanopticHead(nn.Module):\n","    def __init__(self, feature_dim, num_classes, mask_dim):\n","        super(PanopticHead, self).__init__()\n","        self.classifier = nn.Linear(feature_dim, num_classes)\n","        self.mask_predictor = nn.Sequential(\n","            nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(feature_dim, mask_dim, kernel_size=1)\n","        )\n","\n","    def forward(self, features):\n","        classes = self.classifier(features)\n","        masks = self.mask_predictor(features.unsqueeze(-1).unsqueeze(-1))\n","        return classes, masks\n","\n","class DETR(nn.Module):\n","    def __init__(self, num_classes, num_queries, hidden_dim=256, num_heads=8, num_encoder_layers=6, num_decoder_layers=6):\n","        super(DETR, self).__init__()\n","        self.transformer = Transformer(num_encoder_layers, hidden_dim, num_heads, hidden_dim * 4)\n","        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n","        self.panoptic_head = PanopticHead(hidden_dim, num_classes, mask_dim=1)\n","\n","    def forward(self, src, mask):\n","        # src: (batch_size, channel, height, width)\n","        bs, c, h, w = src.size()\n","        src = src.flatten(2).permute(2, 0, 1)  # (hw, bs, c)\n","        pos_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)  # positional embeddings\n","        tgt = torch.zeros_like(pos_embed)\n","        hs = self.transformer(src, tgt + pos_embed)\n","        class_logits, masks = self.panoptic_head(hs[-1])\n","        return class_logits, masks.view(bs, h, w)"]}]}